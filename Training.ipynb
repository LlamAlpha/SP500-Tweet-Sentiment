{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9ec6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1f8902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True, max_length=128)\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer(text['text'],padding='max_length',truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8ab3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='vinai/bertweet-base', vocab_size=64000, model_max_len=128, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c7b2235",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_remaining_09042020_16072020.csv',sep=';')\n",
    "# df1 = df[df['label']=='POS'].sample(500)\n",
    "# df2 = df[df['label']=='NEU'].sample(500)\n",
    "# df3 = df[df['label']=='NEG']\n",
    "# df = pd.concat([df1,df2,df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d5697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'full_text':'text'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab682e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()[['text','label']].reset_index(drop=True)\n",
    "df.rename(columns={'label':'labels'},inplace=True)\n",
    "df['labels'] = df['labels'].replace({'POS':2,'NEU':1,'NEG':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c04162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 2046\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset  = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a61720be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 1534\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 512\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.shuffle().train_test_split()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fa5c0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45f6b7a180848cfb273e3e997199a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f6991b738e4821b95ec1c93969790f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1534\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 512\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize, batched=True)\n",
    "tokenized_datasets.remove_columns(['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c64d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle()\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d2f426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/fine-tuned/\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a512f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ba6d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e38f6762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",per_device_train_batch_size=2,\n",
    "                                 learning_rate=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ace8cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1532fb5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\alexs\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1534\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2301\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2301' max='2301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2301/2301 03:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.960425</td>\n",
       "      <td>0.777344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.613400</td>\n",
       "      <td>1.090936</td>\n",
       "      <td>0.769531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.390200</td>\n",
       "      <td>1.112448</td>\n",
       "      <td>0.783203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_trainer\\checkpoint-500\n",
      "Configuration saved in test_trainer\\checkpoint-500\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-500\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 512\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer\\checkpoint-1000\n",
      "Configuration saved in test_trainer\\checkpoint-1000\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer\\checkpoint-1500\n",
      "Configuration saved in test_trainer\\checkpoint-1500\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-1500\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 512\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer\\checkpoint-2000\n",
      "Configuration saved in test_trainer\\checkpoint-2000\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-2000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 512\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2301, training_loss=0.6034370697565257, metrics={'train_runtime': 184.6019, 'train_samples_per_second': 24.929, 'train_steps_per_second': 12.465, 'total_flos': 302711987096064.0, 'train_loss': 0.6034370697565257, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aebc903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model('fine-tuned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cec312c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 512\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6253' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 18:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.774144172668457,\n",
       " 'eval_accuracy': 0.81640625,\n",
       " 'eval_runtime': 3.1385,\n",
       " 'eval_samples_per_second': 163.134,\n",
       " 'eval_steps_per_second': 20.392}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6841a866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7aadb6a8563d5ef1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\alexs\\.cache\\huggingface\\datasets\\csv\\default-7aadb6a8563d5ef1\\0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871872ff727f44b3b8913b7cd53a2fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97065421d14740c0a41f3a8831245e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\alexs\\.cache\\huggingface\\datasets\\csv\\default-7aadb6a8563d5ef1\\0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "predict_df = pd.read_csv('filtered.csv')\n",
    "predict_ds = Dataset.from_csv('filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0235549",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57cf2ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef5a45e26644c949e25e587705a921d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/461837 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_predict = predict_ds.map(tokenize,batch_size=2,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "998c2d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: created_at, id, text. If created_at, id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 923673\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='115460' max='115460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115460/115460 53:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = trainer.predict(tokenized_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "460f0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = np.argmax(prediction.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce53160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_label'] = predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d41e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Labelled_old_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea3ec6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df.to_csv('Sentiment_Predicted.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
